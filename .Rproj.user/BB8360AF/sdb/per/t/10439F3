{
    "collab_server" : "",
    "contents" : "## Reginald Edwards\n## 26 June 2018\n##\n## Time series analysis via deep learning\nrm(list=ls())\ngc()\n\n# Core Tidyverse\nlibrary(tidyverse)\nlibrary(glue)\nlibrary(forcats)\n\n# Time Series\nlibrary(timetk)\nlibrary(tidyquant)\nlibrary(tibbletime)\n\n# Visualization\nlibrary(cowplot)\n\n# Preprocessing\nlibrary(recipes)\n\n# Sampling / Accuracy\nlibrary(rsample)\nlibrary(yardstick) \n\n# Modeling\nlibrary(keras)\nlibrary(tfruns)\n\n## Run the first time\n#install_keras()\n\n###############################################################################\n\n## sunspot.month is a ts class (not tidy), so we’ll convert to a tidy data set \n## using the tk_tbl() function from timetk. We use this instead of as.tibble() \n## from tibble to automatically preserve the time series index as a zoo yearmon\n## index. Last, we’ll convert the zoo index to date using lubridate::as_date() \n## (loaded with tidyquant) and then change to a tbl_time object to make time \n## series operations easier.\n\n\nsun_spots <- datasets::sunspot.month\nsun_spots <- tk_tbl(sun_spots)\nsun_spots <- mutate(sun_spots, index = as_date(index))\nsun_spots <- as_tbl_time(sun_spots, index = index)\n\n###############################################################################\n## EDA\n###############################################################################\n\np1 <- ggplot(data = sun_spots, aes(index, value)) +\n  geom_point(color = palette_light()[[1]], alpha = 0.5) +\n  theme_tq() +\n  labs(title = \"From 1749 to 2013 (Full Data Set)\")\n\np2 <- ggplot(data = filter_time(sun_spots, \"start\" ~ \"1800\"), \n             aes(index, value)) +\n  geom_line(color = palette_light()[[1]], alpha = 0.5) +\n  geom_point(color = palette_light()[[1]]) +\n  geom_smooth(method = \"loess\", span = 0.2, se = FALSE) +\n  theme_tq() +\n  labs(title = \"1749 to 1759 (Zoomed In To Show Changes over the Year)\",\n       caption = \"datasets::sunspot.month\")\n\np_title <- ggdraw() + draw_label(\"Sunspots\", size = 18, fontface = \"bold\", \n  colour = palette_light()[[1]])\n\nplot_grid(p_title, p1, p2, ncol = 1, rel_heights = c(0.1, 1, 1))\n\n\n###############################################################################\n## Time Series Cross-Validation\n###############################################################################\n\n## time dependencies on preceding samples must be preserved. \n## create a cross validation sampling plan by offsetting the window \n## used to select sequential sub-samples. \n\n## rsample package includes facitlities for backtesting on time series. \n## use the rolling_origin() function to create samples designed for time \n## series cross validation.\n\n## The sampling plan we create uses 50 years (initial = 12 x 50 samples)\n## for the training set and ten years (assess = 12 x 10) for the testing\n## (validation) set. We select a skip span of about twenty years\n## (skip = 12 x 20 - 1) to approximately evenly distribute the samples\n## into 6 sets that span the entire 265 years of sunspots history.\n## Last, we select cumulative = FALSE to allow the origin to shift\n## which ensures that models on more recent data are not given an unfair\n## advantage (more observations) over those operating on less recent data.\n## The tibble return contains the rolling_origin_resamples.\n\nperiods_train <- 12*100\nperiods_test  <- 12*50\nskip_span     <- 12*(22 - 1)\n\nrolling_origin_resamples <- rsample::rolling_origin(sun_spots,\n  initial = periods_train,\n  assess = periods_test,\n  cumulative = FALSE,\n  skip = skip_span)\n\n\n###############################################################################\n## VISUALIZING THE BACKTESTING STRATEGY\n###############################################################################\n\n## We can visualize the resamples with two custom functions. \n## The first, plot_split(), plots one of the resampling splits using ggplot2. \n## Note that an expand_y_axis argument is added to expand the date range to \n## the full sun_spots dataset date range. This will become useful when we \n## visualize all plots together.\n\n# Plotting function for a single split\nplot_split <- function(split, expand_y_axis = TRUE, alpha = 1, size = 1, \n                       base_size = 14){\n  \n  # Manipulate data\n  train_tbl <- training(split) %>%\n    add_column(key = \"training\") \n  \n  test_tbl  <- testing(split) %>%\n    add_column(key = \"testing\") \n  \n  data_manipulated <- bind_rows(train_tbl, test_tbl) %>%\n    as_tbl_time(index = index) %>%\n    mutate(key = fct_relevel(key, \"training\", \"testing\"))\n  \n  # Collect attributes\n  train_time_summary <- train_tbl %>%\n    tk_index() %>%\n    tk_get_timeseries_summary()\n  \n  test_time_summary <- test_tbl %>%\n    tk_index() %>%\n    tk_get_timeseries_summary()\n  \n  # Visualize\n  g <- data_manipulated %>%\n    ggplot(aes(x = index, y = value, color = key)) +\n    geom_line(size = size, alpha = alpha) +\n    theme_tq(base_size = base_size) +\n    scale_color_tq() +\n    labs(\n      title    = glue(\"Split: {split$id}\"),\n      subtitle = glue(\"{train_time_summary$start} to {test_time_summary$end}\"),\n      y = \"\", x = \"\"\n    ) +\n    theme(legend.position = \"none\") \n  \n  if (expand_y_axis) {\n    \n    sun_spots_time_summary <- sun_spots %>% \n      tk_index() %>% \n      tk_get_timeseries_summary()\n    \n    g <- g +\n      scale_x_date(limits = c(sun_spots_time_summary$start, \n                              sun_spots_time_summary$end))\n  }\n  \n  return(g)\n}\n\n\n## The plot_split() function takes one split (in this case Slice01), \n## and returns a visual of the sampling strategy. We expand the axis to \n## the range for the full dataset using expand_y_axis = TRUE.\nrolling_origin_resamples$splits[[1]] %>%\n  plot_split(expand_y_axis = TRUE) +\n  theme(legend.position = \"bottom\")\n\n## The second function, plot_sampling_plan(), scales the plot_split() \n## function to all of the samples using purrr and cowplot.\n\n# Plotting function that scales to all splits \nplot_sampling_plan <- function(sampling_tbl, expand_y_axis = TRUE, ncol = 3, \n  alpha = 1, size = 1, base_size = 14, title = \"Sampling Plan\"){\n  \n  # Map plot_split() to sampling_tbl\n  sampling_tbl_with_plots <- sampling_tbl %>%\n    mutate(gg_plots = map(splits, plot_split, \n                          expand_y_axis = expand_y_axis,\n                          alpha = alpha, base_size = base_size))\n  \n  # Make plots with cowplot\n  plot_list <- sampling_tbl_with_plots$gg_plots \n  \n  p_temp <- plot_list[[1]] + theme(legend.position = \"bottom\")\n  legend <- get_legend(p_temp)\n  \n  p_body  <- plot_grid(plotlist = plot_list, ncol = ncol)\n  \n  p_title <- ggdraw() + \n    draw_label(title, size = 14, fontface = \"bold\", colour = palette_light()[[1]])\n  \n  g <- plot_grid(p_title, p_body, legend, ncol = 1, rel_heights = c(0.05, 1, 0.05))\n  \n  return(g)\n  \n}\n\n\n## We can now visualize the entire backtesting strategy with \n## plot_sampling_plan(). We can see how the sampling plan shifts the sampling \n## window with each progressive slice of the train/test splits.\nrolling_origin_resamples %>%\n  plot_sampling_plan(expand_y_axis = T, ncol = 3, alpha = 1, size = 1, base_size = 10, \n    title = \"Backtesting Strategy: Rolling Origin Sampling Plan\")\n\n\n###############################################################################\n## The LSTM model\n###############################################################################\n\n## To begin, we’ll develop an LSTM model on a single sample from the \n## backtesting strategy, namely, the most recent slice. We’ll then apply the \n## model to all samples to investigate modeling performance.\n\nexample_split    <- rolling_origin_resamples$splits[[6]]\nexample_split_id <- rolling_origin_resamples$id[[6]]\n\n## visualize the split\nplot_split(example_split, expand_y_axis = FALSE, size = 0.5) +\n  theme(legend.position = \"bottom\") +\n  ggtitle(glue(\"Split: {example_split_id}\"))\n\n## Training and Validation Sets\n## To aid hyperparameter tuning, besides the training set we also need a \n## validation set. For example, we will use a callback, \n##callback_early_stopping, that stops training when no significant performance\n## is seen on the validation set (what’s considered significant is up to you).\n## We will dedicate 2 thirds of the analysis set to training, and 1 third to \n## validation.\n\ndf_trn <- analysis(example_split)[1:800, , drop = FALSE]\ndf_val <- analysis(example_split)[801:1200, , drop = FALSE]\ndf_tst <- assessment(example_split)\n\n## First, let’s combine the training and testing data sets into a single data \n## set with a column key that specifies where they came from (either \n## “training” or “testing)”. \n## Note that the tbl_time object will need to have \n## the index respecified during the bind_rows() step\n\ndf <- bind_rows(df_trn %>% add_column(key = \"training\"),\n  df_val %>% add_column(key = \"validation\"),\n  df_tst %>% add_column(key = \"testing\")) %>%\n  as_tbl_time(index = index)\n\n## PREPROCESSING WITH RECIPES\n## The LSTM algorithm will usually work better if the input data has been\n## centered and scaled. We can conveniently accomplish this using the \n## recipes package. In addition to step_center and step_scale, we’re using \n## step_sqrt to reduce variance and remov outliers. \n## The actual transformations are executed when we bake the data according \n## to the recipe:\n\nrec_obj <- recipe(value ~ ., df) %>%\n  step_sqrt(value) %>%\n  step_center(value) %>%\n  step_scale(value) %>%\n  prep()\n\ndf_processed_tbl <- bake(rec_obj, df)\n\n## capture the original center and scale so we can invert the steps after \n## modeling\n\ncenter_history <- rec_obj$steps[[2]]$means[\"value\"]\nscale_history  <- rec_obj$steps[[3]]$sds[\"value\"]\nc(\"center\" = center_history, \"scale\" = scale_history)\n\n## RESHAPING THE DATA\n## Input has to be a 3-d array of size num_samples, num_timesteps, \n## num_features:\n## -- num_samples is the number of observations in the set. This will get fed\n##    to the model in portions of batch_size. \n## -- num_timesteps, is the length of the hidden state \n## -- num_features is the number of predictors we’re using. For univariate \n##    time series, this is 1.\n\n## We want to produce predictions for 12 months so our LSTM should have\n## a hidden state length of 12.\n## These 12 time steps will then get wired to 12 linear predictor units \n## using a time_distributed() wrapper. \n## That wrapper’s task is to apply the same calculation \n## (i.e., the same weight matrix) to every state input it receives.\n\n## As we’re forecasting several timesteps the target data needs to be \n## 3-dimensional. \n## Dimension 1 is the batch dimension, \n## dimension 2 is the number of timesteps (the forecasted ones)\n## dimension 3 is the size of the wrapped layer. \n## The wrapped layer is a layer_dense() of a single unit, as we want exactly \n## one prediction per point in time.\n\n## Create sliding windows of 12 steps of input, followed by 12 steps of \n## output each. \n\nn_timesteps <- 12\nn_predictions <- n_timesteps\nbatch_size <- 10\n\n# functions used\nbuild_matrix <- function(tseries, overall_timesteps) {\n  t(sapply(1:(length(tseries) - overall_timesteps + 1), function(x) \n    tseries[x:(x + overall_timesteps - 1)]))\n}\n\nreshape_X_3d <- function(X) {\n  dim(X) <- c(dim(X)[1], dim(X)[2], 1)\n  X\n}\n\n# extract values from data frame\ntrain_vals <- df_processed_tbl %>%\n  filter(key == \"training\") %>%\n  select(value) %>%\n  pull()\nvalid_vals <- df_processed_tbl %>%\n  filter(key == \"validation\") %>%\n  select(value) %>%\n  pull()\ntest_vals <- df_processed_tbl %>%\n  filter(key == \"testing\") %>%\n  select(value) %>%\n  pull()\n\n\n# build the windowed matrices\ntrain_matrix <-\n  build_matrix(train_vals, n_timesteps + n_predictions)\nvalid_matrix <-\n  build_matrix(valid_vals, n_timesteps + n_predictions)\ntest_matrix <- build_matrix(test_vals, n_timesteps + n_predictions)\n\n# separate matrices into training and testing parts\n# also, discard last batch if there are fewer than batch_size samples\n# (a purely technical requirement)\nX_train <- train_matrix[, 1:n_timesteps]\ny_train <- train_matrix[, (n_timesteps + 1):(n_timesteps * 2)]\nX_train <- X_train[1:(nrow(X_train) %/% batch_size * batch_size), ]\ny_train <- y_train[1:(nrow(y_train) %/% batch_size * batch_size), ]\n\nX_valid <- valid_matrix[, 1:n_timesteps]\ny_valid <- valid_matrix[, (n_timesteps + 1):(n_timesteps * 2)]\nX_valid <- X_valid[1:(nrow(X_valid) %/% batch_size * batch_size), ]\ny_valid <- y_valid[1:(nrow(y_valid) %/% batch_size * batch_size), ]\n\nX_test <- test_matrix[, 1:n_timesteps]\ny_test <- test_matrix[, (n_timesteps + 1):(n_timesteps * 2)]\nX_test <- X_test[1:(nrow(X_test) %/% batch_size * batch_size), ]\ny_test <- y_test[1:(nrow(y_test) %/% batch_size * batch_size), ]\n# add on the required third axis\nX_train <- reshape_X_3d(X_train)\nX_valid <- reshape_X_3d(X_valid)\nX_test <- reshape_X_3d(X_test)\n\ny_train <- reshape_X_3d(y_train)\ny_valid <- reshape_X_3d(y_valid)\ny_test <- reshape_X_3d(y_test)\n\n## Building the LSTM Model\nFLAGS <- flags(\n  # There is a so-called \"stateful LSTM\" in Keras. \n  ## While LSTM is stateful per se,\n  ## this adds a further tweak where the hidden states get initialized with \n  ## values \n  ## from the item at same position in the previous batch.\n  ## This is helpful just under specific circumstances, or if you want to \n  ## create an\n  ## \"infinite stream\" of states, in which case you'd use 1 as the batch size.\n  flag_boolean(\"stateful\", FALSE),\n  \n  ## Should we use several layers of LSTM?\n  ## Again, just included for completeness, it did not yield any superior \n  ## performance on this task.\n  ## This will actually stack exactly one additional layer of LSTM units.\n  flag_boolean(\"stack_layers\", FALSE),\n  \n  ## number of samples fed to the model in one go\n  flag_integer(\"batch_size\", 10),\n  \n  ## size of the hidden state, equals size of predictions\n  flag_integer(\"n_timesteps\", 12),\n  \n  ## how many epochs to train for\n  flag_integer(\"n_epochs\", 100),\n  \n  ## fraction of the units to drop for the linear transformation of the inputs\n  flag_numeric(\"dropout\", 0.2),\n  \n  ## fraction of units to drop for the linear transformation of the recurrent \n  ## state\n  flag_numeric(\"recurrent_dropout\", 0.2),\n  \n  ## loss function: better for this specific case than mean squared error\n  flag_string(\"loss\", \"logcosh\"),\n  \n  ## optimizer = stochastic gradient descent. \n  ## Seemed to work better than adam or rmsprop\n  flag_string(\"optimizer_type\", \"sgd\"),\n  \n  ## size of the LSTM layer\n  flag_integer(\"n_units\", 128),\n  \n  ## learning rate\n  flag_numeric(\"lr\", 0.003),\n  \n  ## momentum, an additional parameter to the SGD optimizer\n  flag_numeric(\"momentum\", 0.9),\n  \n  ## parameter to the early stopping callback\n  flag_integer(\"patience\", 10))\n\n## the number of predictions we'll make equals the length of the hidden state\nn_predictions <- FLAGS$n_timesteps\n\n## how many features = predictors we have\nn_features <- 1\n\n## just in case we wanted to try different optimizers, we could add here\noptimizer <- switch(FLAGS$optimizer_type, sgd = optimizer_sgd(lr = FLAGS$lr, \n  momentum = FLAGS$momentum))\n\n## callbacks to be passed to the fit() function\n## We just use one here: we may stop before n_epochs if the loss on the \n## validation set does not decrease (by a configurable amount, over a \n## configurable time)\ncallbacks <- list(callback_early_stopping(patience = FLAGS$patience))\n\n\n###############################################################################\n## “long version”, would allow you to test stacking several LSTMs or use a \n## stateful LSTM\n###############################################################################\n\nmodel <- keras_model_sequential()\n\nmodel %>%\n  layer_lstm(\n    units = FLAGS$n_units,\n    batch_input_shape  = c(FLAGS$batch_size, FLAGS$n_timesteps, n_features),\n    dropout = FLAGS$dropout,\n    recurrent_dropout = FLAGS$recurrent_dropout,\n    return_sequences = TRUE,\n    stateful = FLAGS$stateful\n  )\n\nif (FLAGS$stack_layers) {\n  model %>%\n    layer_lstm(\n      units            = FLAGS$n_units,\n      dropout = FLAGS$dropout,\n      recurrent_dropout = FLAGS$recurrent_dropout,\n      return_sequences = TRUE,\n      stateful = FLAGS$stateful\n    )\n}\nmodel %>% time_distributed(layer_dense(units = 1))\n\nmodel %>%\n  compile(\n    loss = FLAGS$loss,\n    optimizer = optimizer,\n    metrics = list(\"mean_squared_error\")\n  )\n\nif (!FLAGS$stateful) {\n  model %>% fit(\n    x          = X_train,\n    y          = y_train,\n    validation_data = list(X_valid, y_valid),\n    batch_size = FLAGS$batch_size,\n    epochs     = FLAGS$n_epochs,\n    callbacks = callbacks\n  )\n  \n} else {\n  for (i in 1:FLAGS$n_epochs) {\n    model %>% fit(\n      x          = X_train,\n      y          = y_train,\n      validation_data = list(X_valid, y_valid),\n      callbacks = callbacks,\n      batch_size = FLAGS$batch_size,\n      epochs     = 1,\n      shuffle    = FALSE\n    )\n    model %>% reset_states()\n  }\n}\n\nif (FLAGS$stateful) model %>% reset_states()\n\n\n## Now let’s step through the simpler, yet better (or equally)\n## performing configuration below.\n\n## create the model\nmodel <- keras_model_sequential()\n\n## add layers\n## we have just two, the LSTM and the time_distributed \nmodel %>%\n  layer_lstm(\n    units = FLAGS$n_units, \n    # the first layer in a model needs to know the shape of the input data\n    batch_input_shape  = c(FLAGS$batch_size, FLAGS$n_timesteps, n_features),\n    dropout = FLAGS$dropout,\n    recurrent_dropout = FLAGS$recurrent_dropout,\n    # by default, an LSTM just returns the final state\n    return_sequences = TRUE\n  ) %>% time_distributed(layer_dense(units = 1))\n\nmodel %>%\n  compile(loss = FLAGS$loss,\n    optimizer = optimizer,\n    # in addition to the loss, Keras will inform us about current MSE while training\n    metrics = list(\"mean_squared_error\"))\n\nhistory <- model %>% fit(\n  x          = X_train,\n  y          = y_train,\n  validation_data = list(X_valid, y_valid),\n  batch_size = FLAGS$batch_size,\n  epochs     = FLAGS$n_epochs,\n  callbacks = callbacks)\n\nplot(history, metrics = \"loss\")\n",
    "created" : 1530036902449.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2705715963",
    "id" : "10439F3",
    "lastKnownWriteTime" : 1530042232,
    "last_content_update" : 1530042232456,
    "path" : "C:/Users/Reginald/Dropbox/Research/ceres/code/deep-learning-time-series.R",
    "project_path" : "code/deep-learning-time-series.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 12,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}